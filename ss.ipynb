{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, sys, time, yaml\n",
    "os.environ['DISPLAY'] = 'localhost:10.0'\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "\n",
    "from collections import deque\n",
    "from numpy.linalg import inv\n",
    "import pickle\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pipenv run python -m snakeviz program.prof`\n",
    "\n",
    "`pipenv run python -m cProfile -o program.prof map_creators2.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.one import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = '/home/alex_odnakov/datasets/kitti_dataset'\n",
    "sequences_dir = os.path.join(dataset, \"sequences\")\n",
    "sequence_folders = [\n",
    "    f for f in sorted(os.listdir(sequences_dir))\n",
    "    if os.path.isdir(os.path.join(sequences_dir, f))\n",
    "]\n",
    "\n",
    "seq_i=1\n",
    "i=0\n",
    "scan_list = {}\n",
    "labels_list = {}\n",
    "for i in sequence_folders:\n",
    "    scan_list[i] = sorted(os.listdir(os.path.join(sequences_dir,i,'velodyne')))\n",
    "    if os.path.isdir(os.path.join(sequences_dir,i,'labels')):\n",
    "        labels_list[i] = sorted(os.listdir(os.path.join(sequences_dir,i,'labels')))\n",
    "i = 400\n",
    "\n",
    "\n",
    "a = SemLaserScan(color_dict, project=True)\n",
    "\n",
    "a.open_scan(os.path.join(sequences_dir,sequence_folders[seq_i],'velodyne',scan_list[sequence_folders[seq_i]][i]))\n",
    "a.open_label(os.path.join(sequences_dir,sequence_folders[seq_i],'labels',labels_list[sequence_folders[seq_i]][i]))\n",
    "a.colorize()\n",
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.two import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_path = '/home/alex_odnakov/personal/dataset/sequences'\n",
    "calibration = parse_calibration(os.path.join(second_path,sequence_folders[seq_i], \"calib.txt\"))\n",
    "poses = parse_poses(os.path.join(second_path,sequence_folders[seq_i], \"poses.txt\"), calibration)\n",
    "poses = np.hstack(poses)\n",
    "# diffs = []\n",
    "# for post in posses:\n",
    "#     diff = np.matmul(inv(pose), past[\"pose\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pose' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39mmatmul(inv(\u001b[43mpose\u001b[49m), poses)[:,i\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m:i\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m4\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pose' is not defined"
     ]
    }
   ],
   "source": [
    "np.matmul(inv(pose), poses)[:,i*4:i*4+4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 31/4541 [00:01<04:36, 16.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop has been terminated.\n"
     ]
    }
   ],
   "source": [
    "history = deque()\n",
    "seq_i = 0\n",
    "all_labels = []\n",
    "try:\n",
    "    for i, bin_name in enumerate(tqdm(scan_list[sequence_folders[seq_i]])):\n",
    "        scan_filename = os.path.join(sequences_dir,sequence_folders[seq_i], \"velodyne\", bin_name)\n",
    "        scan = np.fromfile(scan_filename, dtype=np.float32)\n",
    "\n",
    "        scan = scan.reshape((-1, 4))\n",
    "\n",
    "        label_filename = os.path.join(sequences_dir,sequence_folders[seq_i], \"labels\", os.path.splitext(bin_name)[0] + \".label\")\n",
    "        labels = np.fromfile(label_filename, dtype=np.uint32)\n",
    "        labels = labels.reshape((-1))\n",
    "        all_labels.append(labels)\n",
    "        # convert points to homogenous coordinates (x, y, z, 1)\n",
    "        points = np.ones((scan.shape))\n",
    "        points[:, 0:3] = scan[:, 0:3]\n",
    "        remissions = scan[:, 3]\n",
    "\n",
    "        pose = poses[i]\n",
    "\n",
    "        num_concat_points = points.shape[0]\n",
    "        num_concat_points += sum([past[\"points\"].shape[0] for past in history])\n",
    "        concated_points = np.zeros((num_concat_points * 4), dtype = np.float32)\n",
    "        \n",
    "        start = 0\n",
    "        concated_points[4 * start:4 * (start + points.shape[0])] = scan.reshape((-1))\n",
    "        start += points.shape[0]\n",
    "\n",
    "        for past in history:\n",
    "            diff = np.matmul(inv(pose), past[\"pose\"])\n",
    "            tpoints = np.matmul(diff, past[\"points\"].T).T\n",
    "            tpoints[:, 3] = past[\"remissions\"]\n",
    "            tpoints = tpoints.reshape((-1))\n",
    "\n",
    "            concated_points[4 * start:4 * (start + past[\"points\"].shape[0])] = tpoints\n",
    "            start += past[\"points\"].shape[0]   \n",
    "\n",
    "        # append current data to history queue.\n",
    "        history.appendleft({\n",
    "            \"points\": points,\n",
    "            \"remissions\": remissions,\n",
    "            \"pose\": pose.copy()\n",
    "        })\n",
    " \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Loop has been terminated.\")\n",
    "all_labels = np.hstack(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = SemLaserScan(color_dict, project=False)\n",
    "a.set_points(concated_points.reshape((-1,4))[:,:3],concated_poin\n",
    "             ts.reshape((-1,4))[:,:4])\n",
    "a.set_label(all_labels)\n",
    "a.colorize()\n",
    "# a.show(voxel_size=0.5)\n",
    "a.save(os.path.join(sequences_dir,sequence_folders[seq_i]),'whole_map',voxel_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    }
   ],
   "source": [
    "a = SemLaserScan(color_dict, project=False)\n",
    "a.set_points(concated_points.reshape((-1,4))[:,:3],concated_poin\n",
    "             ts.reshape((-1,4))[:,:4])\n",
    "a.set_label(concated_labels)\n",
    "a.colorize()\n",
    "# a.show(voxel_size=0.5)\n",
    "a.save(os.path.join(sequences_dir,sequence_folders[seq_i]),'whole_map',voxel_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = SemLaserScan(color_dict, project=False)\n",
    "a.open_scan(os.path.join(sequences_dir,sequence_folders[seq_i],'whole_map.bin'))\n",
    "a.open_label(os.path.join(sequences_dir,sequence_folders[seq_i],'whole_map.label'))\n",
    "a.colorize()\n",
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1101/1101 [00:19<00:00, 57.21it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_kitti_scan(file_path):\n",
    "    scan = np.fromfile(file_path, dtype=np.float32)\n",
    "    return scan.reshape((-1, 4))\n",
    "\n",
    "def transform_cloud_to_world_frame(xyz, pose):\n",
    "    points = np.hstack((xyz, np.ones((xyz.shape[0], 1), dtype=xyz.dtype)))\n",
    "    transformed_points = pose @ points.T\n",
    "\n",
    "    return transformed_points.T[:,:3]\n",
    "\n",
    "def build_kitti_map(base_path):\n",
    "\n",
    "    # Load KITTI poses\n",
    "    poses = parse_poses(os.path.join(base_path,\"poses.txt\"), parse_calibration(os.path.join(base_path,\"calib.txt\")))\n",
    "    universal_scan = SemLaserScan(color_dict)\n",
    "    all_points = []\n",
    "    all_remissions = []\n",
    "    all_labels = []\n",
    "    for idx, pose in enumerate(tqdm(poses)):\n",
    "        scan_filename = os.path.join(base_path, \"velodyne\", f'{idx:06d}.bin')\n",
    "        label_filename = os.path.join(base_path, \"labels\", f'{idx:06d}.label')\n",
    "        \n",
    "        if not os.path.isfile(scan_filename):\n",
    "            continue\n",
    "        universal_scan.open_scan(scan_filename)\n",
    "        universal_scan.open_label(label_filename)\n",
    "        \n",
    "        transformed_scan = transform_cloud_to_world_frame(universal_scan.points, pose)\n",
    "        \n",
    "        # if idx==0:\n",
    "        #     all_points = universal_scan.points\n",
    "        #     all_remissions = universal_scan.remissions\n",
    "        #     all_labels = universal_scan.sem_label\n",
    "        #     continue\n",
    "        \n",
    "        new_points = transform_cloud_to_world_frame(universal_scan.points, pose)\n",
    "        new_remissions = universal_scan.remissions\n",
    "        new_labels = universal_scan.sem_label\n",
    "        all_points.append(new_points)\n",
    "        all_remissions.append(new_remissions)\n",
    "        all_labels.append(new_labels)\n",
    "        \n",
    "    all_points = np.vstack(all_points)\n",
    "    all_remissions = np.hstack(all_remissions)\n",
    "    all_labels = np.hstack(all_labels)\n",
    "    universal_scan.set_points(all_points, all_remissions)\n",
    "    universal_scan.set_label(all_labels)\n",
    "\n",
    "    return universal_scan\n",
    "\n",
    "# set your KITTI point cloud directory and poses file\n",
    "base_path = \"/home/aleksadnr/diplom/dataset/sequences/01\"\n",
    "\n",
    "# build the global map\n",
    "global_cloud = build_kitti_map(base_path)\n",
    "\n",
    "# visualize the global map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable             Memory Size     Type      \n",
      "---------------------------------------------\n",
      "poses                0.55 MB         ndarray   \n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "def show_global_var_sizes():\n",
    "    global_vars = globals()\n",
    "    print(\"{:<20} {:<15} {:<10}\".format('Variable', 'Memory Size', 'Type'))\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    # Exclude built-in modules and special variables\n",
    "    excluded_names = set(dir(__builtins__)) | {'In', 'Out', 'get_ipython', 'exit', 'quit'}\n",
    "    \n",
    "    for name, obj in global_vars.items():\n",
    "        if name not in excluded_names and not name.startswith('_'):\n",
    "            size = sys.getsizeof(obj)\n",
    "            obj_type = type(obj).__name__\n",
    "            if size>1024*128:\n",
    "                print(\"{:<20} {:<15} {:<10}\".format(name, f'{(size//1024)/1024:.2f} MB', obj_type))\n",
    "show_global_var_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "global_cloud.colorize()\n",
    "global_cloud.voxel_ds(voxel_size=0.5)\n",
    "# global_cloud.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp.pkl', 'wb') as file: \n",
    "    pickle.dump(global_cloud, file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp1.pkl', 'rb') as file: \n",
    "    global_cloud  = pickle.load(file) \n",
    "global_cloud.colorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_cloud.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PointCloud with 1620694 points."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(global_cloud.points)\n",
    "pcd.colors = o3d.utility.Vector3dVector(global_cloud.sem_label_color)\n",
    "downpcd = pcd.voxel_down_sample(voxel_size=0.5)\n",
    "downpcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116355734, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_cloud.sem_label_color.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def voxel_ds(self=global_cloud, voxel_size=None):\n",
    "    xyz = self.points\n",
    "    labels = self.sem_label\n",
    "    remissions = self.remissions\n",
    "   \n",
    "    if self.voxel_size is None:\n",
    "        self.voxel_size = voxel_size\n",
    "    indices = np.floor(xyz / self.voxel_size).astype(np.int32)\n",
    "    unique_indices, unique_inverse = np.unique(indices, return_inverse=True, axis=0)\n",
    "    num_unique_voxels = len(unique_indices)\n",
    "    self.unique_inverse = unique_inverse\n",
    "    print('openning loop')\n",
    "    results = []\n",
    "    for idx in tqdm(range(num_unique_voxels)):\n",
    "        results.append(self._downsample(idx))\n",
    "    downsampled_xyz, downsampled_remissions, downsampled_labels = zip(*results)\n",
    "    \n",
    "    self.ds_points = np.vstack(downsampled_xyz)\n",
    "    self.ds_remissions = np.hstack(downsampled_remissions)\n",
    "\n",
    "    if len(downsampled_labels)== self.ds_points.shape[0]:\n",
    "      self.ds_sem_label = np.asarray(downsampled_labels) & 0xFFFF  # semantic label in lower half\n",
    "      self.ds_inst_label = np.asarray(downsampled_labels) >> 16    # instance id in upper half\n",
    "    \n",
    "    self.colorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_cloud.voxel_ds = voxel_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11635573,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "global_cloud.sem_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = SemLaserScan(color_dict, project=True)\n",
    "\n",
    "a.open_scan(\"/home/aleksadnr/diplom/dataset/sequences/01/whole_map_0.01ds_0.1vs.bin\")\n",
    "a.open_label(\"/home/aleksadnr/diplom/dataset/sequences/01/whole_map_0.01ds_0.1vs.label\")\n",
    "a.colorize()\n",
    "a.show(voxel=True,voxel_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done colarization\n"
     ]
    }
   ],
   "source": [
    "global_cloud.colorize()\n",
    "print('done colarization')\n",
    "global_cloud.show(sample_rate=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw-_LjAFJo6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
